{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa1fa57-404a-450d-b41c-3c4398c3050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  693k  100  693k    0     0   588k      0  0:00:01  0:00:01 --:--:--  589k\n",
      "Archive:  swe-eng.zip\n",
      "  inflating: _about.txt              \n",
      "  inflating: swe.txt                 \n"
     ]
    }
   ],
   "source": [
    "!curl -O http://www.manythings.org/anki/swe-eng.zip\n",
    "!unzip swe-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b59d9-c29d-4196-95d8-da0b7e4bfaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: packaging in /Users/nils/Library/Python/3.9/lib/python/site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (3.19.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/nils/Library/Python/3.9/lib/python/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa100823-2712-4336-a3b9-d4f5a9630a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.10.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip3 show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb783c-3945-4baf-bd60-0f24022d3d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (3.19.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in /Users/nils/Library/Python/3.9/lib/python/site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/nils/Library/Python/3.9/lib/python/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9267077-9178-4098-b34a-5e67bd209ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:24:00.980916: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca7b1db-023f-4583-9fe1-74565664adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'swe.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8ad908-011a-498f-a9f8-b3c4bff75b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'eng'\n",
    "TRG_LANGUAGE = 'swe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6cc0610-88ff-4858-82d5-33fc859848d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform = {}\n",
    "vocab_transform = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853f5972-0ee6-43c4-a1a1-3a5cfbc63931",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 1000000 # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e05697a-2385-43f7-95d8-532aa2ecc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eabf8130-ec27-445f-8e3e-bb6c6cc42089",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"[start] \" + target_text + \" [end]\"\n",
    "    text_pairs.append((input_text, target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd18285f-8384-4877-af6f-dd125172b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('She speaks quickly.', '[start] Hon talar snabbt. [end]')\n",
      "('Two children are sitting on the fence.', '[start] Två barn sitter på staketet. [end]')\n",
      "('These are very fragile.', '[start] De här är väldigt sköra. [end]')\n",
      "('Can you measure the length?', '[start] Kan du mäta längden? [end]')\n",
      "('Those are free.', '[start] De där är gratis. [end]')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f92409be-f3ed-4906-a82d-01552df38ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22187 total pairs\n",
      "15531 training pairs\n",
      "3328 validation pairs\n",
      "3328 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b87badf0-431d-4fbf-ae46-b0fbbe686c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41176f8-b084-4b2c-9d5a-ae64ff1571e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408b61bf-843e-4439-b615-9fa541ef1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:25:11.282376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a5cf743-d1ad-4a1d-9d0c-2402fbcf8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2a7f74-cc13-4257-bc49-1270f161b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f00d12b-6605-49c8-b2c9-4cd48a2c1a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_6 (Positi  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_3 (Transfo  (None, None, 256)   3155456     ['positional_embedding_6[0][0]'] \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_7 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:27:42.416103: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/transformer/transformer_encoder_3/multi_head_attention_9/softmax/add/BroadcastGradientArgs' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/bq/myp7pv7d2kx50tbcnmg3pjgm0000gn/T/ipykernel_15550/619779574.py\", line 7, in <module>\n      transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 577, in minimize\n      loss, var_list=var_list, grad_loss=grad_loss, tape=tape\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 635, in _compute_gradients\n      tape, loss, var_list, grad_loss\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/transformer/transformer_encoder_3/multi_head_attention_9/softmax/add/BroadcastGradientArgs'\nIncompatible shapes: [64,8,20,20] vs. [64,64,20,20]\n\t [[{{node gradient_tape/transformer/transformer_encoder_3/multi_head_attention_9/softmax/add/BroadcastGradientArgs}}]] [Op:__inference_train_function_26485]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bq/myp7pv7d2kx50tbcnmg3pjgm0000gn/T/ipykernel_15550/619779574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"rmsprop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/transformer/transformer_encoder_3/multi_head_attention_9/softmax/add/BroadcastGradientArgs' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/bq/myp7pv7d2kx50tbcnmg3pjgm0000gn/T/ipykernel_15550/619779574.py\", line 7, in <module>\n      transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 577, in minimize\n      loss, var_list=var_list, grad_loss=grad_loss, tape=tape\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 635, in _compute_gradients\n      tape, loss, var_list, grad_loss\n    File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/transformer/transformer_encoder_3/multi_head_attention_9/softmax/add/BroadcastGradientArgs'\nIncompatible shapes: [64,8,20,20] vs. [64,64,20,20]\n\t [[{{node gradient_tape/transformer/transformer_encoder_3/multi_head_attention_9/softmax/add/BroadcastGradientArgs}}]] [Op:__inference_train_function_26485]"
     ]
    }
   ],
   "source": [
    "epochs = 1  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a675ae-62b8-457b-9ef1-3f7b4c63913f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
