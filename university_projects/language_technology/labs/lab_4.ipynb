{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques: Keras Version\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. You will train your models with either Keras or PyTorch. Choose one of these framework following the name of the notebook.\n",
    "\n",
    "Before you start the assignment, please run the prerequisites from the prerequistites notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect partial syntactic structures called groups or chunks\n",
    "* Understand the principles of supervised machine learning techniques applied to language processing\n",
    "* Use a popular machine learning toolkit: either Keras or PyTorch\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version\n",
      "--------------------------------- -----------\n",
      "absl-py                           1.2.0\n",
      "altair                            4.2.0\n",
      "appdirs                           1.4.4\n",
      "appnope                           0.1.2\n",
      "argon2-cffi                       20.1.0\n",
      "astunparse                        1.6.3\n",
      "async-generator                   1.10\n",
      "attrs                             21.2.0\n",
      "backcall                          0.2.0\n",
      "bleach                            3.3.0\n",
      "blinker                           1.4\n",
      "cachetools                        5.2.0\n",
      "certifi                           2021.5.30\n",
      "cffi                              1.14.5\n",
      "charset-normalizer                2.0.12\n",
      "click                             8.1.3\n",
      "commonmark                        0.9.1\n",
      "conlleval                         0.2\n",
      "cycler                            0.11.0\n",
      "debugpy                           1.3.0\n",
      "decorator                         5.0.9\n",
      "defusedxml                        0.7.1\n",
      "distlib                           0.3.2\n",
      "entrypoints                       0.3\n",
      "filelock                          3.0.12\n",
      "flatbuffers                       22.9.24\n",
      "fonttools                         4.33.3\n",
      "gast                              0.4.0\n",
      "gitdb                             4.0.9\n",
      "GitPython                         3.1.27\n",
      "google-auth                       2.12.0\n",
      "google-auth-oauthlib              0.4.6\n",
      "google-pasta                      0.2.0\n",
      "grpcio                            1.49.1\n",
      "h5py                              3.7.0\n",
      "idna                              3.3\n",
      "importlib-metadata                4.11.4\n",
      "ipykernel                         6.0.1\n",
      "ipython                           7.25.0\n",
      "ipython-genutils                  0.2.0\n",
      "ipywidgets                        7.6.3\n",
      "jedi                              0.18.0\n",
      "Jinja2                            3.0.1\n",
      "joblib                            1.1.0\n",
      "jsonschema                        3.2.0\n",
      "jupyter                           1.0.0\n",
      "jupyter-client                    6.1.12\n",
      "jupyter-console                   6.4.0\n",
      "jupyter-contrib-core              0.4.0\n",
      "jupyter-contrib-nbextensions      0.5.1\n",
      "jupyter-core                      4.7.1\n",
      "jupyter-highlight-selected-word   0.2.0\n",
      "jupyter-latex-envs                1.4.6\n",
      "jupyter-nbextensions-configurator 0.5.0\n",
      "jupyterlab-pygments               0.1.2\n",
      "jupyterlab-widgets                1.0.0\n",
      "keras                             2.10.0\n",
      "Keras-Preprocessing               1.1.2\n",
      "kiwisolver                        1.4.3\n",
      "libclang                          14.0.6\n",
      "lxml                              4.9.1\n",
      "Markdown                          3.4.1\n",
      "MarkupSafe                        2.1.1\n",
      "matplotlib                        3.5.2\n",
      "matplotlib-inline                 0.1.2\n",
      "mistune                           0.8.4\n",
      "nbclient                          0.5.3\n",
      "nbconvert                         6.1.0\n",
      "nbformat                          5.1.3\n",
      "nest-asyncio                      1.5.1\n",
      "notebook                          6.4.0\n",
      "numpy                             1.22.4\n",
      "oauthlib                          3.2.1\n",
      "opencv-python                     4.6.0.66\n",
      "opt-einsum                        3.3.0\n",
      "packaging                         21.0\n",
      "pandas                            1.4.2\n",
      "pandocfilters                     1.4.3\n",
      "parso                             0.8.2\n",
      "pexpect                           4.8.0\n",
      "pickleshare                       0.7.5\n",
      "Pillow                            9.1.1\n",
      "pip                               22.2.2\n",
      "plotly                            5.8.2\n",
      "prometheus-client                 0.11.0\n",
      "prompt-toolkit                    3.0.19\n",
      "protobuf                          3.19.5\n",
      "ptyprocess                        0.7.0\n",
      "pyarrow                           8.0.0\n",
      "pyasn1                            0.4.8\n",
      "pyasn1-modules                    0.2.8\n",
      "pycparser                         2.20\n",
      "pydeck                            0.7.1\n",
      "pyee                              7.0.4\n",
      "Pygments                          2.9.0\n",
      "Pympler                           1.0.1\n",
      "pyparsing                         2.4.7\n",
      "pyppeteer                         0.2.2\n",
      "pyrsistent                        0.18.0\n",
      "python-dateutil                   2.8.1\n",
      "pytz                              2022.1\n",
      "pytz-deprecation-shim             0.1.0.post0\n",
      "PyYAML                            6.0\n",
      "pyzmq                             22.1.0\n",
      "qtconsole                         5.1.1\n",
      "QtPy                              1.9.0\n",
      "regex                             2022.8.17\n",
      "requests                          2.27.1\n",
      "requests-oauthlib                 1.3.1\n",
      "rich                              12.4.4\n",
      "rsa                               4.9\n",
      "scikit-learn                      1.1.1\n",
      "scipy                             1.8.1\n",
      "seaborn                           0.11.2\n",
      "semver                            2.13.0\n",
      "Send2Trash                        1.7.1\n",
      "setuptools                        56.0.0\n",
      "six                               1.16.0\n",
      "sklearn                           0.0\n",
      "smmap                             5.0.0\n",
      "streamlit                         1.10.0\n",
      "streamlit-drawable-canvas         0.9.1\n",
      "tenacity                          8.0.1\n",
      "tensorboard                       2.10.1\n",
      "tensorboard-data-server           0.6.1\n",
      "tensorboard-plugin-wit            1.8.1\n",
      "tensorflow                        2.10.0\n",
      "tensorflow-estimator              2.10.0\n",
      "tensorflow-io-gcs-filesystem      0.27.0\n",
      "termcolor                         2.0.1\n",
      "terminado                         0.10.1\n",
      "testpath                          0.5.0\n",
      "threadpoolctl                     3.1.0\n",
      "toml                              0.10.2\n",
      "toolz                             0.11.2\n",
      "torch                             1.11.0\n",
      "torchvision                       0.12.0\n",
      "tornado                           6.1\n",
      "tqdm                              4.64.1\n",
      "traitlets                         5.0.5\n",
      "typing_extensions                 4.2.0\n",
      "tzdata                            2022.1\n",
      "tzlocal                           4.2\n",
      "urllib3                           1.26.9\n",
      "validators                        0.19.0\n",
      "virtualenv                        20.4.7\n",
      "wcwidth                           0.2.5\n",
      "webencodings                      0.5.1\n",
      "websockets                        8.1\n",
      "Werkzeug                          2.2.2\n",
      "wheel                             0.37.1\n",
      "widgetsnbextension                3.5.1\n",
      "wrapt                             1.14.1\n",
      "zipp                              3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!conda install pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conlleval\n",
      "  Using cached conlleval-0.2-py3-none-any.whl (5.4 kB)\n",
      "Installing collected packages: conlleval\n",
      "Successfully installed conlleval-0.2\n"
     ]
    }
   ],
   "source": [
    "! /Users/nils/opt/anaconda3/bin/pip install conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeds\n",
    "Making things reproduceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "keras.utils.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "LSTM_HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the paths to load the datasets from your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'corpus/train.txt'\n",
    "test_file = 'corpus/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now convert the dataset in a Python data structure. Read the functions below to load the datasets. They store the corpus in a list of sentences. Each sentence is a list of rows, where each row is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 files have three columns: The wordform, `form`, its part of speech, `pos`, and the tag denoting the syntactic group also called the chunk tag, `chunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus as a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'He', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'reckons', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'current', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'account', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'will', 'pos': 'MD', 'chunk': 'B-VP'},\n",
       "  {'form': 'narrow', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'only', 'pos': 'RB', 'chunk': 'B-NP'},\n",
       "  {'form': '#', 'pos': '#', 'chunk': 'I-NP'},\n",
       "  {'form': '1.8', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'billion', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_dict = split_rows(train_sentences, column_names)\n",
    "train_dict[10:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'corpus/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function below that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the embeddings\n",
    "embeddings_dict = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# words in embedding dictionary: 400000'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'# words in embedding dictionary: {}'.format(len(embedded_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chording',\n",
       " 'chordoma',\n",
       " 'chordophones',\n",
       " 'chords',\n",
       " 'chore',\n",
       " 'chorea',\n",
       " 'chorene',\n",
       " 'choreograph',\n",
       " 'choreographed',\n",
       " 'choreographer']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_words[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.51973,  1.0395 ,  0.20924,  0.16285,  0.7209 ,  0.81524,\n",
       "       -0.34641, -0.76654, -0.49576,  0.24634,  0.44094,  0.37701,\n",
       "       -0.16396,  0.2775 ,  0.16563,  0.43869, -1.0887 ,  0.12663,\n",
       "        0.66916,  0.3578 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['chords'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a cosine similarity, write a `closest(target_word, embeddings, count=10)` that computes the 10 closest words to the words _table_, _france_, and _sweden_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from heapq import nlargest\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "def closest(target_word, embeddings, count = 10):\n",
    "    cosines = {other_word : cosine(embeddings.get(target_word), embedding) for other_word, embedding in embeddings.items()}\n",
    "    cosines = sorted(cosines, reverse = True, key = lambda x: cosines[x])[0:10]\n",
    "    return cosines\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['table', 'tables', 'place', 'bottom', 'room', 'side', 'sit', 'top', 'here', 'pool'], ['france', 'belgium', 'french', 'britain', 'spain', 'paris', 'germany', 'italy', 'europe', 'netherlands'], ['sweden', 'denmark', 'norway', 'finland', 'netherlands', 'austria', 'switzerland', 'germany', 'swedish', 'belgium']]\n"
     ]
    }
   ],
   "source": [
    "target_words = ['table', 'france', 'sweden']\n",
    "print([closest(w, embeddings_dict) for w in target_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the $\\mathbf{X}$ and $\\mathbf{Y}$ Lists of Symbols from the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, you will build an input sequence, $\\mathbf{x}$, corresponding to the words and an output one, $\\mathbf{y}$, corresponding to the chunk tags.\n",
    "\n",
    "Write a `build_sequences(corpus_dict, key_x='form', key_y='chunk', tolower=True)` function that, for each sentence, returns the $\\mathbf{x}$ and $\\mathbf{y}$ lists of symbols consisting of words and chunk tags. Set the words in lower case if `tolower` is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 11th sentence of the training set, you should have:<br/>\n",
    "`x = ['he',  'reckons',  'the',  'current',  'account',  'deficit',  'will',  'narrow',  'to',  'only',  '#',  '1.8',  'billion',  'in',  'september',  '.']`\n",
    "\n",
    "`y = ['B-NP', 'B-VP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'I-VP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'O']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='chunk', tolower=True):\n",
    "    xs = [[word[key_x].lower() if tolower else word[key_x] for word in sample] for sample in corpus_dict] \n",
    "    ys = [[word[key_y] for word in sample] for sample in corpus_dict] \n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_symbs, Y_train_symbs = build_sequences(train_dict, key_x='form', key_y='chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'reckons', 'the', 'current', 'account', 'deficit', 'will', 'narrow', 'to', 'only', '#', '1.8', 'billion', 'in', 'september', '.']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_symbs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-NP', 'B-VP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'I-VP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_symbs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vocabulary of all the words observed in the training set as well as in GloVe. You should find 401,464 different words. You will proceed in two steps.\n",
    "\n",
    "First extract the list of unique words `words` from the CoNLL training set and the list of chunk tags, `chunks`. You will sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code: List of words and tags in CoNLL\n",
    "words = sorted(list(set([item for sublist in X_train_symbs for item in sublist])))\n",
    "chunks = sorted(list(set([item for sublist in Y_train_symbs for item in sublist])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words seen in training corpus: 17258\n",
      "# Chunks tags seen: 22\n"
     ]
    }
   ],
   "source": [
    "print('# words seen in training corpus:', len(words))\n",
    "print('# Chunks tags seen:', len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['casinos',\n",
       " 'caspita',\n",
       " 'caspita-brand',\n",
       " 'cassettes',\n",
       " 'cast',\n",
       " 'castigated',\n",
       " 'castigating',\n",
       " 'castillo',\n",
       " 'casting',\n",
       " 'castro-medellin']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[4000:4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ADJP',\n",
       " 'B-ADVP',\n",
       " 'B-CONJP',\n",
       " 'B-INTJ',\n",
       " 'B-LST',\n",
       " 'B-NP',\n",
       " 'B-PP',\n",
       " 'B-PRT',\n",
       " 'B-SBAR',\n",
       " 'B-UCP']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, merge the list of unique CoNLL words with the words in the embeddings file. You will sort this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code: Add vocabulary of embedded words\n",
    "vocabulary_words = sorted(list(set(embedded_words + words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in the vocabulary: embeddings and corpus: 401464\n"
     ]
    }
   ],
   "source": [
    "print('# words in the vocabulary: embeddings and corpus:', len(vocabulary_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy',\n",
       " 'joya',\n",
       " 'joyal',\n",
       " 'joyandet',\n",
       " 'joyas',\n",
       " 'joyce',\n",
       " 'joycean',\n",
       " 'joycelyn',\n",
       " 'joyces',\n",
       " 'joydeep']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words[200000:200010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the indices `word2idx`, `chunk2idx` and inverted indices `idx2word`, `idx2chunk` for the words and the chunk tags: i.e. you will associate each word with a number. You will use index 0 for the padding symbol and 1 for unknown words. This means that your first word will start at index 2. For the chunks, you will start at index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code:\n",
    "idx2word = dict(list(enumerate(vocabulary_words, start = 2)))\n",
    "idx2chunk = dict(list(enumerate(chunks, start = 1)))\n",
    "word2idx = {v : k for k,v in idx2word.items()}\n",
    "chunk2idx = {v : k for k,v in idx2chunk.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11), ('###', 12), ('#a', 13), ('#aabccc', 14), ('#b', 15), ('#c', 16), ('#cc', 17), ('#ccc', 18), ('#cccccc', 19), ('#ccccff', 20), ('#d', 21), ('#daa', 22), ('#dcdcdc', 23), ('#e', 24), ('#f', 25), ('#faf', 26)]\n"
     ]
    }
   ],
   "source": [
    "print(list(word2idx.items())[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B-ADJP', 1), ('B-ADVP', 2), ('B-CONJP', 3), ('B-INTJ', 4), ('B-LST', 5), ('B-NP', 6), ('B-PP', 7), ('B-PRT', 8), ('B-SBAR', 9), ('B-UCP', 10), ('B-VP', 11), ('I-ADJP', 12), ('I-ADVP', 13), ('I-CONJP', 14), ('I-INTJ', 15), ('I-NP', 16), ('I-PP', 17), ('I-PRT', 18), ('I-SBAR', 19), ('I-UCP', 20), ('I-VP', 21), ('O', 22)]\n"
     ]
    }
   ],
   "source": [
    "print(list(chunk2idx.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy matrix of dimensions $(M, N)$, where $M$ will be the size of the vocabulary: The unique words in the training set and the words in GloVe, and $N$, the dimension of the embeddings.\n",
    "The padding symbol and the unknown word symbol will be part of the vocabulary at respectively index 0 and 1. \n",
    "\n",
    "Initialize the matrix with random values with the `np.random.uniform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add two dimensions for the padding symbol at index 0 and unknown words at index 1\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "# embedding_matrix = np.random.random((len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "# embedding_matrix = np.zeros((len(vocabulary_words) + 2, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of your matrix is: (401466, 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401466, 100)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the matrix with the GloVe embeddings when available. This means: Replace the random vector with an embedding when available. You will use the indices from the previous section. You will call `out_of_embeddings` the list of words in CoNLL, but not in the embedding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "out_of_embeddings = sorted(list(set(words) - set(embedded_words)))\n",
    "for word, idx in word2idx.items():\n",
    "    if word in embeddings_dict:\n",
    "        embedding_matrix[idx] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"y'all\",\n",
       " 'yankus',\n",
       " 'year-ago',\n",
       " 'year-before',\n",
       " 'year-earlier',\n",
       " 'year-to-date',\n",
       " 'yield-management',\n",
       " 'zaishuo',\n",
       " 'zarett',\n",
       " 'zumbrunn']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_embeddings[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of the padding symbol, idx 0, random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0060013 , -0.04786904, -0.03472253, -0.03932863, -0.01280712,\n",
       "       -0.00589381, -0.01308496,  0.0305994 , -0.02799489, -0.03747122])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of the word _table_, the GloVe values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61453998,  0.89692998,  0.56770998,  0.39102   , -0.22437   ,\n",
       "        0.49035001,  0.10868   ,  0.27410999, -0.23833001, -0.52152997])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[word2idx['table']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of _zarett_, a word in CoNLL 2000, but not in GloVe, random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04988765, -0.03493965,  0.01686851, -0.00719784,  0.02015915,\n",
       "       -0.02578659,  0.04145163,  0.0088572 ,  0.02801581, -0.04028402])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[word2idx['zarett']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the $\\mathbf{X}$ and $\\mathbf{Y}$ Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the input and output sequences with numerical indices. First, convert the \n",
    "$\\mathbf{X}_\\text{train\\_symbs}$ and $\\mathbf{Y}_\\text{train\\_symbs}$ \n",
    "lists of symbols in lists of numbers using the indices you created. Call them `X_train_idx` and `Y_train_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "X_train_idx = [[word2idx[word] for word in sample] for sample in X_train_symbs]\n",
    "Y_train_idx = [[chunk2idx[word] for word in sample] for sample in Y_train_symbs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word indices of the three first sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107701, 189360, 358640, 291209, 193879, 388606, 143496, 362305, 353285, 56501, 328878, 126632, 187522, 364843, 148777, 152124, 326524, 454, 131007, 152124, 306232, 363097, 454, 144953, 362305, 331257, 43426, 347508, 189267, 155109, 200552, 55175, 63614, 154, 259236, 120001, 873], [97171, 269136, 358640, 143112, 262191, 219534, 154, 307829, 106548, 362305, 43426, 149626, 249511, 288933, 174855, 177388, 362305, 293204, 43426, 154301, 189360, 344283, 274536, 358640, 279589, 386150, 873], [88319, 54890, 304156, 372747, 349558, 152124, 344283, 174855, 72318, 139858, 88675, 358640, 97171, 154, 144970, 362305, 56361, 57639, 261034, 288933, 240241, 189360, 180283, 234487, 183252, 340448, 218722, 360423, 873]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_idx[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk tag indices of the three first sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7, 6, 16, 11, 21, 21, 21, 21, 6, 16, 16, 9, 6, 16, 7, 6, 22, 1, 7, 6, 6, 22, 11, 21, 21, 6, 16, 16, 7, 6, 16, 16, 6, 16, 16, 22], [22, 7, 6, 16, 6, 16, 6, 16, 16, 7, 6, 16, 16, 16, 11, 21, 21, 21, 6, 16, 7, 6, 7, 6, 16, 16, 22], [22, 6, 11, 6, 16, 7, 6, 11, 21, 21, 7, 6, 16, 6, 16, 11, 21, 6, 16, 16, 16, 7, 6, 16, 16, 16, 6, 16, 22]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_idx[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pad the sentences using the `pad_sequences` function from Keras. After padding, the second sentence you look like (the indices are not necessarily the same).\n",
    "```\n",
    "x = [ 97171, 269136, 358640, 143112, 262191, 219534,    154, 307829,\n",
    "       106548, 362305,  43426, 149626, 249511, 288933, 174855, 177388,\n",
    "       362305, 293204,  43426, 154301, 189360, 344283, 274536, 358640,\n",
    "       279589, 386150,    873,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0]\n",
    "y = [22,  7,  6, 16,  6, 16,  6, 16, 16,  7,  6, 16, 16, 16, 11, 21, 21,\n",
    "       21,  6, 16,  7,  6,  7,  6, 16, 16, 22,  0,  0,  0,  0,  0,  0,  0,\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "```\n",
    "\n",
    "You will call the results `X_train_padded` and `Y_train_padded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "X_train_padded = pad_sequences(X_train_idx, padding = 'post')\n",
    "Y_train_padded = pad_sequences(Y_train_idx, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97171, 269136, 358640, 143112, 262191, 219534,    154, 307829,\n",
       "       106548, 362305,  43426, 149626, 249511, 288933, 174855, 177388,\n",
       "       362305, 293204,  43426, 154301, 189360, 344283, 274536, 358640,\n",
       "       279589, 386150,    873,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0], dtype=int32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22,  7,  6, 16,  6, 16,  6, 16, 16,  7,  6, 16, 16, 16, 11, 21, 21,\n",
       "       21,  6, 16,  7,  6,  7,  6, 16, 16, 22,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_padded[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the indices in the $\\mathbf{Y}_\\text{train\\_padded}$ vector into one-hot encoded vectors. Use `to_categorical()`. Call the result `Y_train_padded_vectorized`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "Y_train_padded_vectorized = to_categorical(Y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_padded_vectorized[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your network consisting of one embedding layer, a simple recurrent neural network, either RNN or LSTM, and a dense layer. You will initialize the embedding layer with `embedding_matrix`. You will set the embeddings as nontrainable first. You may try other configurations after. As number of RNN/LSTM units use 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras' from '/Users/nils/opt/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/__init__.py'>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "import tensorflow as tf\n",
    "model1 = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = embedding_matrix.shape[0],\n",
    "                                                       output_dim = embedding_matrix.shape[1],\n",
    "                                                       embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                                       trainable = False,\n",
    "                                                       input_length= None,\n",
    "                                                       mask_zero = True),\n",
    "                             tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "                             tf.keras.layers.Dense(len(chunks) + 1, activation = 'softmax')\n",
    "                            ])\n",
    "model1.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile your code with a loss, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, None, 100)         40146600  \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, None, 256)         365568    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, None, 23)          5911      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,518,079\n",
      "Trainable params: 371,479\n",
      "Non-trainable params: 40,146,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "280/280 [==============================] - 101s 333ms/step - loss: 0.2513 - accuracy: 0.7545\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 84s 299ms/step - loss: 0.1288 - accuracy: 0.8739\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 90s 320ms/step - loss: 0.1021 - accuracy: 0.8992\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 92s 328ms/step - loss: 0.0874 - accuracy: 0.9142\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 84s 300ms/step - loss: 0.0774 - accuracy: 0.9243\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 106s 377ms/step - loss: 0.0702 - accuracy: 0.9311\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 86s 307ms/step - loss: 0.0642 - accuracy: 0.9367\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 78s 278ms/step - loss: 0.0592 - accuracy: 0.9417\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 84s 300ms/step - loss: 0.0544 - accuracy: 0.9466\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 82s 294ms/step - loss: 0.0503 - accuracy: 0.9501\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train_padded, Y_train_padded_vectorized, epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we visualize the training curves. Ideally, we would compare them with those of a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.title('Training accuracies')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model. (This takes a long time as you have to save the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.save(\"keras_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = keras.models.load_model(\"keras_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the model on a test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The United States might collapsez'.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentence words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "sentence_word_idxs = [word2idx[w] if w in word2idx else 1 for w in sentence]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices. Note the 1 at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ['the', 'united', 'states', 'might', 'collapsez']\n",
      "Sentence word indexes [358640, 373606, 343335, 245002, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence', sentence)\n",
    "print('Sentence word indexes', sentence_word_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the chunks. Call the variable `sent_chunk_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "sent_chunk_predictions = model1.predict([sentence_word_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 23)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_chunk_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated probabilities of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.8673696e-07, 1.7764591e-05, 3.0283685e-04, 2.1060423e-06,\n",
       "       2.5845711e-06, 1.5754720e-06, 9.9852341e-01, 7.3398754e-04,\n",
       "       1.1441005e-05, 1.5425043e-04, 2.5825986e-07, 8.7327080e-06,\n",
       "       5.8786486e-07, 1.2575285e-06, 1.7351095e-06, 1.0515313e-06,\n",
       "       1.0665933e-04, 8.9973946e-06, 1.1065702e-06, 8.1253330e-07,\n",
       "       5.0150948e-07, 2.9164738e-07, 1.1784492e-04], dtype=float32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_chunk_predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply argmax to select the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: B-NP\n",
      "united: I-NP\n",
      "states: I-NP\n",
      "might: B-VP\n",
      "collapsez /ukn: I-VP\n"
     ]
    }
   ],
   "source": [
    "for word_nbr, chunk_predictions in enumerate(sent_chunk_predictions[0]):\n",
    "    if sentence_word_idxs[word_nbr] in idx2word:\n",
    "        print(idx2word[sentence_word_idxs[word_nbr]], end=': ')\n",
    "    else:\n",
    "        print(sentence[word_nbr], '/ukn', end=': ')\n",
    "    print(idx2chunk.get(np.argmax(chunk_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_dict = split_rows(test_sentences, column_names)\n",
    "test_dict[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the $\\mathbf{X}$ and $\\mathbf{Y}$ sequences of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test: ['rockwell', 'said', 'the', 'agreement', 'calls', 'for', 'it', 'to', 'supply', '200', 'additional', 'so-called', 'shipsets', 'for', 'the', 'planes', '.']\n",
      "Y_test ['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'B-SBAR', 'B-NP', 'B-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "X_test_symbs, Y_test_symbs = build_sequences(test_dict, key_x='form', key_y='chunk')\n",
    "print('X_test:', X_test_symbs[1])\n",
    "print('Y_test', Y_test_symbs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the $\\mathbf{X}$ symbol sequence into an index sequence and pad it. Call the results `X_test_idx` and `X_test_padded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "X_test_idx = [[word2idx[word] if word in word2idx else 1 for word in sample] for sample in X_test_symbs]\n",
    "X_test_padded = pad_sequences(X_test_idx, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_idx: [311438, 316957, 358640, 48789, 90494, 152124, 194623, 362305, 349553, 17495, 46648, 337426, 1, 152124, 358640, 287224, 873]\n",
      "X_test_padded: [311438 316957 358640  48789  90494 152124 194623 362305 349553  17495\n",
      "  46648 337426      1 152124 358640 287224    873      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "print('X_test_idx:', X_test_idx[1])\n",
    "print('X_test_padded:', X_test_padded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2012, 70)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the chunks. Call the result `Y_test_hat_probs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 9s 112ms/step\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "Y_test_hat_probs = model1.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions [[1.4478670e-05 5.6036981e-05 1.1067592e-04 ... 1.0991365e-04\n",
      "  1.3199328e-04 2.2577338e-03]\n",
      " [2.1804633e-08 1.8830716e-06 3.0083009e-05 ... 3.2907803e-07\n",
      "  2.2525483e-05 2.4125176e-04]\n",
      " [1.9371176e-09 4.8540392e-06 6.3606412e-06 ... 8.9781782e-09\n",
      "  7.3422848e-07 2.1366817e-05]\n",
      " ...\n",
      " [3.0910300e-08 1.1541104e-05 1.4160571e-04 ... 1.5548193e-06\n",
      "  4.0949749e-07 9.9839044e-01]\n",
      " [3.0910300e-08 1.1541104e-05 1.4160571e-04 ... 1.5548193e-06\n",
      "  4.0949749e-07 9.9839044e-01]\n",
      " [3.0910300e-08 1.1541104e-05 1.4160571e-04 ... 1.5548193e-06\n",
      "  4.0949749e-07 9.9839044e-01]]\n"
     ]
    }
   ],
   "source": [
    "print('Predictions', Y_test_hat_probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'B-ADJP',\n",
       " 2: 'B-ADVP',\n",
       " 3: 'B-CONJP',\n",
       " 4: 'B-INTJ',\n",
       " 5: 'B-LST',\n",
       " 6: 'B-NP',\n",
       " 7: 'B-PP',\n",
       " 8: 'B-PRT',\n",
       " 9: 'B-SBAR',\n",
       " 10: 'B-UCP',\n",
       " 11: 'B-VP',\n",
       " 12: 'I-ADJP',\n",
       " 13: 'I-ADVP',\n",
       " 14: 'I-CONJP',\n",
       " 15: 'I-INTJ',\n",
       " 16: 'I-NP',\n",
       " 17: 'I-PP',\n",
       " 18: 'I-PRT',\n",
       " 19: 'I-SBAR',\n",
       " 20: 'I-UCP',\n",
       " 21: 'I-VP',\n",
       " 22: 'O'}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict the whole test set and we store the results in each dictionary with the key `pchunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, y_hat_probs in zip(test_dict, Y_test_hat_probs):\n",
    "    sent_len = len(sent)\n",
    "    y_hat_probs = y_hat_probs[:sent_len]\n",
    "    y_hat = map(np.argmax, y_hat_probs)\n",
    "    for word, ner_hat in zip(sent, y_hat):\n",
    "        word['pchunk'] = idx2chunk[ner_hat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence example: `chunk` is the hand annotation and `pchunk` is the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       " {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR', 'pchunk': 'B-PP'},\n",
       " {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       " {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       " {'form': '200', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the test set in a file to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk', 'pchunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    with open(file, 'w', encoding='utf8') as f_out:\n",
    "        i += 1\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += ' '.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'test_model1.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(outfile, test_dict, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8584242724867726"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(outfile, encoding='utf8').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "chunker_score = res['overall']['chunks']['evals']['f1']\n",
    "chunker_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8983791168981722 bidi lstm non trainable 10 epochs\n",
    "# 0.8954656530002717 bidi lstm non trainable 15 epochs\n",
    "# 0.9038243480053603 bidi lstm trainable 10 epochs\n",
    "# 0.9071813322795999 bidi lstm trainable 15 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will carry out experiments with two different recurrent networks: RNN and LSTM. You will also try at least two sets of parameters per network. In your report, you will present your results in a table like this one:\n",
    "\n",
    "|Method|Parameters|Score|\n",
    "|------|-----|-----|\n",
    "|Baseline|  selecting the chunk tag which was most frequently associated with the current part-of-speech tag | 0.771 |\n",
    "|RNN|  hidden_units=256, dropout=0.5 | 0.832 |\n",
    "|RNN |  hidden_units=64, dropout=0.5 | 0.785 |\n",
    "|LSTM |  hidden_units=256, dropout=0.5 | 0.859 |\n",
    "|LSTM |  hidden_units=64, dropout=0.5 | 0.811 |\n",
    "|  Akbik et al.|  xx| 0.967 |\n",
    "\n",
    "The baseline is the one from the CoNLL 2000 site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"ni5324ro-s\", \"si7660da-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"lab_4.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of your best score and the confirmed entities. You need a score of more than 88 to pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"chunker_score\": 0.8584242724867726}'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'chunker_score': chunker_score})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 4\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'incorrect',\n",
       " 'signature': None,\n",
       " 'submission_id': '3e770ca2-6622-49bf-927f-8384499ac20f'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will:\n",
    "1. Write a short individual report on your program. You will describe the architecture your used the different experiments you carried out and your results.\n",
    "2. Read the article, <a href=\"https://www.aclweb.org/anthology/C18-1139\"><i>Contextual String Embeddings for Sequence Labeling</i></a> by Akbik et al. (2018) and outline the main differences between their system and yours. A LSTM is a type of recurrent neural network, while CRF is a sort of beam search. You will tell the performance they reach on the corpus you used in this laboratory.\n",
    "\n",
    "Submit your report as well as your notebook (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 7, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
